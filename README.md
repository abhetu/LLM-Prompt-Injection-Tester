# ğŸ›¡ï¸ LLM Prompt Injection Tester

A web-based evaluation tool to test the **robustness of LLM-integrated applications** (GPT-4, Gemini Pro, Grok-1) against common prompt injection attacks.

---

## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ attacks/
â”‚   â””â”€â”€ intentions.ts               # Injection logic and class-based validators
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ TestRunner.tsx             # UI to select test + run against model
â”‚   â”œâ”€â”€ ResultsVisualization.tsx   # Graphs, CSV export, recent results
â”‚   â””â”€â”€ MitigationGuide.tsx        # Optional: defense tips
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sampleData.ts              # 3 sample attack definitions
â”œâ”€â”€ harness/
â”‚   â”œâ”€â”€ OpenAIHarness.ts           # GPT-4 interface via OpenAI API
â”‚   â”œâ”€â”€ GeminiHarness.ts           # Gemini Pro (Google) interface
â”‚   â””â”€â”€ GrokHarness.ts             # xAI Grok API adapter
â”œâ”€â”€ App.tsx                        # Combines UI & visualization
â”œâ”€â”€ main.tsx                       # React entry point
â”œâ”€â”€ types.ts                       # Global type definitions
â””â”€â”€ index.css                      # Styling
```

---

## ğŸš€ Features

- Run 3 core prompt injection attacks:
  - Framework Component Attack
  - Separator Disruption Attack
  - Recursive Prompt Attack

- Evaluate responses from:
  - GPT-4 (OpenAI)
  - Gemini Pro (Google)
  - Grok-1 (xAI)

- Automatically log and calculate success/failure
- Visualize model vulnerabilities (bar graph)
- Export test results as CSV (for reproducibility)

---

## ğŸ§ª Setup & Deployment

### ğŸ–¥ï¸ Local Dev

```bash
git clone https://github.com/YOUR_USERNAME/LLM-Prompt-Injection-Tester.git
cd LLM-Prompt-Injection-Tester
npm install
```
Create a `.env` file and include your API keys:

```
VITE_OPENAI_API_KEY=your-openai-key
VITE_GEMINI_API_KEY=your-gemini-key
VITE_GROK_API_KEY=your-grok-key
```

Then run locally:

```bash
npm run dev
```

### â˜ï¸ Vercel Deployment

- Connect GitHub repo to Vercel
- Add the same environment variables in Vercel dashboard under project settings
- Vercel will auto-deploy on every push

---

## ğŸ§  Research Foundations

- **Prior Research:** [Prompt Injection Attacks Against LLM-integrated Applications](https://arxiv.org/abs/2310.02631) â€” introduces HOUYI framework and threat taxonomy.
- **Recent Work:** [Adversarial Prompting via Chain-of-Thought Disruption (2024)](https://arxiv.org/abs/2401.03947) â€” builds on prior attacks and introduces deeper LLM-aware evasions.

---

## âš ï¸ Disclaimer

This tool is intended for academic/research purposes **only**. Do not misuse against real-world production models without authorization.

---

## ğŸ‘¥ Authors

Built for CS4371 â€” Group 9:
- Anubhav Bhetuwal
- Ananta Aryal
- Balmiki R. Padhyaya
- Sebika Khulal
- Shishir Khanal
